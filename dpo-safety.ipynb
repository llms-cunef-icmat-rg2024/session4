{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vicgalle/configurable-system-prompt-multitask\")[\"train\"]\n",
    "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# add special tokens\n",
    "if model_name == \"teknium/OpenHermes-2.5-Mistral-7B\":\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"</s>\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_prompt(system, prompt):\n",
    "    if system is None:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    else:\n",
    "        if model_name == \"abacusai/bigstral-12b-32k\":\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": system + \"\\n\" + prompt},\n",
    "            ]\n",
    "        else:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system,\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def template_answer(answer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": answer,\n",
    "        },\n",
    "    ]\n",
    "    answer = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "# create new columns\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": template_prompt(x[\"system\"], x[\"prompt\"])\n",
    "    },  # change this according to the dataset!!!\n",
    ")\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"chosen\": template_answer(x[\"chosen\"])},\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"rejected\": template_answer(x[\"rejected\"])},\n",
    ")\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8 * 3,\n",
    "    lora_alpha=16 * 3,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"k_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"v_proj\",\n",
    "        \"up_proj\",\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Model to fine-tune\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = f\"checkpoints/exp_configurable_{model_name}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=True,\n",
    "    output_dir=output_name,\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
